{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fvcore\n",
    "\n",
    "from DDSP.models.ddsp_decoder import DDSP_Decoder\n",
    "from DDSP.models.synths.hpn_synth import *\n",
    "from DDSP.models.decoder.decoders import *\n",
    "from DDSP.utils import build_model\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsp_model = \"hpn\"\n",
    "version = \"full\"\n",
    "config_path = f\"configs/{ddsp_model}/{ddsp_model}_{version}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "decoder_config = config[\"decoder\"]\n",
    "synth_config = config[\"synth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model for analyzing it in terms of embeddability\n",
    "model = build_model(decoder_config=decoder_config, synth_config=synth_config, ddsp_mode=ddsp_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Time with a buffer\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "def inference(model, x):\n",
    "    with torch.no_grad():\n",
    "        model(x)\n",
    "\n",
    "x= {}\n",
    "BUFFER = 512\n",
    "sub_label = str(BUFFER/64)\n",
    "x['audio'] = torch.randn(1, BUFFER, 1)\n",
    "x['f0'] = torch.randn(1, int(BUFFER/64), 1)\n",
    "x['f0_scaled'] = torch.randn(1, int(BUFFER/64), 1)\n",
    "x['loudness_scaled'] = torch.randn(1, int(BUFFER/64), 1)\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "        stmt='inference(model,x)',\n",
    "        globals={'x': x, 'model': model, 'inference': inference},\n",
    "        num_threads=1,\n",
    "        label=f\"TEST with BUFFER={BUFFER}\",\n",
    "        sub_label=sub_label+ \" ms\",\n",
    "        description=f'{ddsp_model}-{version}',\n",
    "    )\n",
    "print(t0.timeit(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_memory = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "if param_memory > 100000:\n",
    "    param_memory_mb = param_memory / (1024 ** 2)  # Convert to MB\n",
    "    print(f\"Model Parameters Memory: {param_memory_mb:.2f} MB\")\n",
    "else:\n",
    "    param_memory_kb = param_memory / (1024)  # Convert to MB\n",
    "    print(f\"Model Parameters Memory: {param_memory_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the model with 16000 samples\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "BUFFER = 16000\n",
    "\n",
    "x = {}\n",
    "x['audio'] = torch.randn(1, BUFFER, 1)\n",
    "x['f0'] = torch.randn(1, int(BUFFER/64), 1)\n",
    "x['f0_hz'] = torch.randn(1, int(BUFFER/64), 1)\n",
    "x['f0_scaled'] = torch.randn(1, int(BUFFER/64), 1)\n",
    "x['loudness_scaled'] = torch.randn(1, int(BUFFER/64), 1)\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, profile_memory=True,  with_flops=True, with_modules=True) as prof_large:\n",
    "    with record_function(\"model_inference\"):\n",
    "        with torch.no_grad():\n",
    "            model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{ddsp_model.upper()}-{version.capitalize()}\")\n",
    "print(prof_large.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLOPs analysis with fvcore\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table, flop_count_str, flop_count_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FLOPs\n",
    "flops = FlopCountAnalysis(model.decoder, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print FLOPs\n",
    "flops.by_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operators not supported by fvcore: GRU and ReLU/LeakyRelu\n",
    "def gru_inference_flops(input_size, hidden_size, sequence_length, num_layers, batch_size):\n",
    "    # FLOPs for one layer (per timestep)\n",
    "    flops_per_timestep = 0\n",
    "    \n",
    "    # FLOPs for update gate z_t\n",
    "    flops_per_timestep += 2 * (input_size * hidden_size + hidden_size * hidden_size)  # W_z * x_t and U_z * h_{t-1}\n",
    "    flops_per_timestep += hidden_size  # Sigmoid activation\n",
    "\n",
    "    # FLOPs for reset gate r_t\n",
    "    flops_per_timestep += 2 * (input_size * hidden_size + hidden_size * hidden_size)  # W_r * x_t and U_r * h_{t-1}\n",
    "    flops_per_timestep += hidden_size  # Sigmoid activation\n",
    "\n",
    "    # FLOPs for candidate hidden state h_tilde\n",
    "    flops_per_timestep += 2 * (input_size * hidden_size + hidden_size * hidden_size)  # W_h * x_t and U_h * h_{t-1}\n",
    "    flops_per_timestep += hidden_size  # r_t * U_h * h_{t-1}\n",
    "    flops_per_timestep += hidden_size  # Tanh activation\n",
    "\n",
    "    # FLOPs for new hidden state h_t\n",
    "    flops_per_timestep += 3 * hidden_size  # (1 - z_t) * h_{t-1} + z_t * h_tilde\n",
    "\n",
    "    # Total FLOPs per timestep for one layer\n",
    "    total_flops_per_timestep = flops_per_timestep\n",
    "\n",
    "    # Total FLOPs for the entire sequence (all layers, all timesteps)\n",
    "    total_flops = total_flops_per_timestep * sequence_length * num_layers * batch_size\n",
    "    print(total_flops)\n",
    "    return total_flops\n",
    "\n",
    "def relu_inference_flops(input_size, sequence_length, batch_size):\n",
    "    # FLOPs for ReLU activation function\n",
    "    print(input_size * sequence_length * batch_size)\n",
    "    return input_size * sequence_length * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_inference_flops(1024, 512, BUFFER/64, 1, 1)\n",
    "relu_inference_flops(512, BUFFER/64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_inference_flops(32, 16, BUFFER/64, 1, 1)\n",
    "relu_inference_flops(16, BUFFER/64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_inference_flops(128, BUFFER/64, 1)\n",
    "relu_inference_flops(64, BUFFER/64, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
